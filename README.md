# 60Days_MachineLearningScientist
![alt text](https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/images/Day0MachineLearningScientist.PNG)


 | Books and Resources |
| ----- |
| 1. [**Machine Learning Scientist with Python (Datacamp)**](https://www.datacamp.com/tracks/machine-learning-scientist-with-python) |
| 2. [**Intro to Machine Learning, Andreas Muller**](https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/Books/Intro%20to%20ML-compressed.pdf) |
| 3. [**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, Aurélien Géron**](https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/Books/Hand-on-ML-compressed.pdf) |

**Day0 of 60DaysOfMachineLearningScientist!**
- **Classification**: Today I learned to implement KNearest Neighbors Algorithm.KNearest Algorithm's basic idea is to predict the label of a data point by looking at the 'k' closest labelled data points  and then taking a majority vote. All the ML models are implemented as Python classes and they implement algorithm for learning and predicting and store the information learned from that data. Training a model on the data is also called "fitting" a model to the data using .fit() method. To predict the labels of new data .predict() method is used.In classification, accuracy is commonly used metric. Accuracy = Fraction of correct predictions. To measure the model performance, we split the data into training and testing set using train_test_split() method and then fit/train the model on the training set and make predictions on test and then compare predictions with the known labels, we use .score() method to do that.Larget valaue of 'k' can lead to less complex model and underfitting and smaller value of 'k' = more complex model and can lead to overfitting. So those are the things that I learned today at Day 0. I am very excited to learn upcoming chapters and share it on the github.
- link to notebook :(https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/Codes%20and%20exercises/1.Supervised%20Learning%20with%20scikit%20learn/supervised_learning_chapter1_classification.ipynb)

**Day1 of 60DaysOfMachineLearningScientist!**
- **Regression**: Today I learned about Regression. In regression tasks, the target value is a continuously varying variable, such as a country's GDP or the price of a house. To use a linear model, we import LinearRegression() from sklearn.linear_model and then instantiate that model then fit the model to the training data and then check the model's prediction over testing data. The basics of linear regression is y = ax + b where y is the target, x is the single feature and a and b are the parameters of the model that we want to learn. When we have two features and one target, a line is of the form y = a1x1 + a2x2 + b, so to fit a linear regression model is to specify three variables, a1, a2, and b. In higher dimensions, that is, when we have more than one or two features, a line of this form, so fitting a linear regression model is to specify a coefficient, ai, for each feature, as well as the variable, b. if you're computing R squared on your test set, the R squared returned is dependent on the way that you split up the data! The data points in the test set may have some peculiarities that mean the R squared computed on it is not representative of the model's ability to generalize to unseen data. To solve this we use, cross-validation technique. The loss or cost function , The vertical distance between fit and the data is called a residual, sum of squares of the residuals is called ordinary least square or OLS for short. When you call fit on a linear regression model in scikit-learn, it performs this OLS under the hood i.e minimizes the sum of the squares of the residuals!. Regularizations is the common practice to alter the loss function so that it penalizes for large coefficent. There are two types of regularized regression, Ridge and Lasso.In Ridge regression ,loss function is the standard OLS loss function plus the squared value of each coefficient multiplied by some constant alpha and in Lasso regression, loss function is the standard OLS loss function plus the absolute value of each coefficient multiplied by some constant alpha.Lasso regression can be used to select important features of a dataset.
- link to notebook:(https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/Codes%20and%20exercises/1.Supervised%20Learning%20with%20scikit%20learn/supervised_learning_chapter2_regression.ipynb)

- **Fine-tuning your model**: After training our model, our next next is to evaluate its performance. In Classification .score() methos is used to evaluate its performance. 2X2 matrix that summarizes predictive performance called a confusion matrix. Precision, which is the number of true positives divided by the total number of true positives and false positives. It is also called the positive predictive value or PPV. Recall, which is the number of true positives divided by the total number of true positives and false negatives. This is also called sensitivity, hit rate, or true positive rate. F1-score is defined as two times the product of the precision and recall divided by the sum of the precision and recall, in other words, it's the harmonic mean of precision and recall. high precision means that our classifier had a low false positive rate, that is, not many real emails were predicted as being spam. Intuitively, high recall means that our classifier predicted most positive or spam emails correctly.
- link to notebook:(https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/Codes%20and%20exercises/1.Supervised%20Learning%20with%20scikit%20learn/supervised_learning_chapter3_fine_tuning_your_model.ipynb)

**Day2 of 60DaysOfMachineLearningScientist!**
- **Preprocessing and Pipeline**
- link to notebook:(https://github.com/bickkysahani/60Days_MachineLearningScientist/blob/main/Codes%20and%20exercises/1.Supervised%20Learning%20with%20scikit%20learn/supervised_learning_chapter4_preprocessing%20and%20pipeline.ipynb)


